# 如何在 S3 上运行 TensorFlow

Tensorflow supports reading and writing data to S3. S3 is an object storage API which is nearly ubiquitous, and can help in situations where data must accessed by multiple actors, such as in distributed training.

This document guides you through the required setup, and provides examples on usage.

## Configuration

你可以通过一系列环境变量来控制你的 TensorFlow 程序在 S3 上读写数据：

*   **AWS_REGION**：S3 默认使用区域终端节点，区域通过 `AWS_REGION` 来设置。如果 `AWS_REGION` 没有被指定，默认会使用 `us-east-1`。
*   **S3_ENDPOINT**：重写 `S3_ENDPOINT` 来明确指定节点。
*   **S3_USE_HTTPS**：S3 默认通过 HTTPS 访问，除非你设置了 `S3_USE_HTTPS=0`。
*   **S3_VERIFY_SSL**：如果你使用了 HTTPS，可以通过 `S3_VERIFY_SSL=0` 来关闭 SSL 校验。

To read or write objects in a bucket that is not publicly accessible, AWS credentials must be provided through one of the following methods:

*   在本地的 AWS 证书文件中设置凭证，Linux、 macOS 和 Unix 存放在 `~/.aws/credentials`，Windows 存放在`C:\Users\USERNAME\.aws\credentials`。
*   设置 `AWS_ACCESS_KEY_ID` 和 `AWS_SECRET_ACCESS_KEY` 环境变量。
*   如果 TensorFlow 部署在一个 EC2 实例上，创建一个 IAM 角色，并赋予该 EC2 实例使用该角色的权限。

## Example Setup

Using the above information, we can configure Tensorflow to communicate to an S3 endpoint by setting the following environment variables:

```bash
AWS_ACCESS_KEY_ID=XXXXX                 # Credentials only needed if connecting to a private endpoint
AWS_SECRET_ACCESS_KEY=XXXXX
AWS_REGION=us-east-1                    # Region for the S3 bucket, this is not always needed. Default is us-east-1.
S3_ENDPOINT=s3.us-east-1.amazonaws.com  # The S3 API Endpoint to connect to. This is specified in a HOST:PORT format.
S3_USE_HTTPS=1                          # Whether or not to use HTTPS. Disable with 0.
S3_VERIFY_SSL=1                         # If HTTPS is used, controls if SSL should be enabled. Disable with 0.
```

## Usage

Once setup is completed, Tensorflow can interact with S3 in a variety of ways. Anywhere there is a Tensorflow IO function, an S3 URL can be used.

### Smoke Test

To test your setup, stat a file:

```python
from tensorflow.python.lib.io import file_io
print file_io.stat('s3://bucketname/path/')
```

You should see output similar to this:

```console
<tensorflow.python.pywrap_tensorflow_internal.FileStatistics; proxy of <Swig Object of type 'tensorflow::FileStatistics *' at 0x10c2171b0> >
```

### Reading Data

When [reading data](../api_guides/python/reading_data.md), change the file paths you use to read and write
data to an S3 path. For example:

```python
filenames = ["s3://bucketname/path/to/file1.tfrecord",
             "s3://bucketname/path/to/file2.tfrecord"]
dataset = tf.data.TFRecordDataset(filenames)
```

### Tensorflow Tools

Many Tensorflow tools, such as Tensorboard or model serving, can also take S3 URLS as arguments:

```bash
tensorboard --logdir s3://bucketname/path/to/model/
tensorflow_model_server --port=9000 --model_name=model --model_base_path=s3://bucketname/path/to/model/export/
```

This enables an end to end workflow using S3 for all data needs.

## S3 Endpoint Implementations

S3 was invented by Amazon, but the S3 API has spread in popularity and has several implementations. The following implementations have passed basic compatibility tests:

* [Amazon S3](https://aws.amazon.com/s3/)
* [Google Storage](https://cloud.google.com/storage/docs/interoperability)
* [Minio](https://www.minio.io/kubernetes.html)
